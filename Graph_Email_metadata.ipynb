{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93882670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class GraphBasedSpamFilter:\n",
    "    def __init__(self, similarity_threshold=0.2,\n",
    "                 min_df=5,\n",
    "                 max_df_ratio=0.8,\n",
    "                 top_k=10):\n",
    "        self.graph = nx.Graph()\n",
    "        self.messages = []               # list of (full_text, label)\n",
    "        self.message_tokens = []         # list of precomputed token sets\n",
    "        self.token_index = defaultdict(set)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.min_df = min_df\n",
    "        self.max_df_ratio = max_df_ratio\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        try:\n",
    "            text = \"\" if text is None else str(text)\n",
    "            text = html.unescape(text.lower())\n",
    "            if \"<\" in text and \">\" in text:\n",
    "                text = BeautifulSoup(text, \"html.parser\").get_text(\" \")\n",
    "            text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "            text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
    "            text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "            text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            clean = [\n",
    "                lemmatizer.lemmatize(w)\n",
    "                for w in tokens\n",
    "                if w not in stop_words and len(w) > 1\n",
    "            ]\n",
    "            return set(clean)\n",
    "        except Exception as e:\n",
    "            print(\"Preprocess error:\", e)\n",
    "            return set()\n",
    "\n",
    "    def _meta_tokens(self, meta: dict):\n",
    "        toks = set()\n",
    "        # domain token\n",
    "        dom = meta.get(\"from_domain\")\n",
    "        if dom:\n",
    "            toks.add(f\"dom_{dom}\")\n",
    "        # attachment\n",
    "        if meta.get(\"has_attachment\", False):\n",
    "            toks.add(\"has_attachment\")\n",
    "        # subject length bucket\n",
    "        sl = meta.get(\"subject_length\", 0)\n",
    "        toks.add(f\"subj_len_{(sl//50)*50}\")\n",
    "        # to_count bucket\n",
    "        tc = meta.get(\"to_count\", 0)\n",
    "        toks.add(f\"to_cnt_{min(tc,10)}\")\n",
    "        # url_count bucket\n",
    "        uc = meta.get(\"url_count\", 0)\n",
    "        toks.add(f\"url_cnt_{min(uc,5)}\")\n",
    "        # hour/weekday\n",
    "        hr = meta.get(\"hour\")\n",
    "        if hr is not None and hr >= 0:\n",
    "            toks.add(f\"hour_{hr}\")\n",
    "            \n",
    "        wd = meta.get(\"weekday\")\n",
    "        if wd is not None and wd >= 0:\n",
    "            toks.add(f\"wkday_{wd}\")\n",
    "        return toks\n",
    "\n",
    "    def cosine_similarity(self, set1, set2):\n",
    "        inter = len(set1 & set2)\n",
    "        return inter / ((len(set1)*len(set2))**0.5 + 1e-9)\n",
    "\n",
    "    def add_message(self, subject, body, label, metadata=None):\n",
    "        # text tokens\n",
    "        subj_toks = self.preprocess(subject)\n",
    "        body_toks = self.preprocess(body)\n",
    "        tokens = subj_toks | body_toks\n",
    "        # metadata tokens\n",
    "        if metadata:\n",
    "            tokens |= self._meta_tokens(metadata)\n",
    "\n",
    "        idx = len(self.messages)\n",
    "        self.messages.append((f\"{subject} {body}\", label))\n",
    "        self.message_tokens.append(tokens)\n",
    "        self.graph.add_node(idx, subject=subject, body=body, label=label)\n",
    "\n",
    "        # connect to prior messages\n",
    "        candidates = {i for t in tokens for i in self.token_index[t]}\n",
    "        for i in candidates:\n",
    "            sim = self.cosine_similarity(tokens, self.message_tokens[i])\n",
    "            if sim > self.similarity_threshold:\n",
    "                self.graph.add_edge(idx, i, weight=sim)\n",
    "        for t in tokens:\n",
    "            self.token_index[t].add(idx)\n",
    "\n",
    "    def predict_spam_subject_body(self, subject, body, metadata=None):\n",
    "        subj_toks = self.preprocess(subject)\n",
    "        body_toks = self.preprocess(body)\n",
    "        tokens = subj_toks | body_toks\n",
    "        if metadata:\n",
    "            tokens |= self._meta_tokens(metadata)\n",
    "\n",
    "        scores = []\n",
    "        candidates = {i for t in tokens for i in self.token_index.get(t, [])}\n",
    "        for i in candidates:\n",
    "            sim = self.cosine_similarity(tokens, self.message_tokens[i])\n",
    "            if sim > self.similarity_threshold:\n",
    "                scores.append(self.messages[i][1])\n",
    "        return 1.0 if sum(scores)/max(1, len(scores)) > 0.5 else 0.0\n",
    "\n",
    "    def train_with_dataframe(self, df):\n",
    "        # Expect df columns: from, to, date, subject, body, label\n",
    "        for _, row in df.iterrows():\n",
    "            meta = {\n",
    "                \"from_domain\": row[\"from\"].split(\"@\")[-1] if pd.notnull(row[\"from\"]) and \"@\" in row[\"from\"] else None,\n",
    "                \"to_count\": len(str(row[\"to\"]).split(\",\")) if pd.notnull(row[\"to\"]) else 0,\n",
    "                \"subject_length\": len(str(row[\"subject\"] or \"\")),\n",
    "                \"has_attachment\": False,\n",
    "                \"hour\": pd.to_datetime(row[\"date\"], errors=\"coerce\").hour if pd.notnull(row[\"date\"]) else None,\n",
    "                \"weekday\": pd.to_datetime(row[\"date\"], errors=\"coerce\").weekday() if pd.notnull(row[\"date\"]) else None,\n",
    "                \"url_count\": len(re.findall(r\"http[s]?://\\S+\", str(row[\"body\"] or \"\")))\n",
    "            }\n",
    "            self.add_message(row[\"subject\"], row[\"body\"], row[\"label\"], metadata=meta)\n",
    "\n",
    "        N = len(self.messages)\n",
    "        # filter tokens by df\n",
    "        df_counts = {t: len(idxs) for t, idxs in self.token_index.items()}\n",
    "        valid = {t for t, c in df_counts.items()\n",
    "                 if c >= self.min_df and c <= self.max_df_ratio * N}\n",
    "\n",
    "        # rebuild index & tokens\n",
    "        self.token_index = defaultdict(set)\n",
    "        for i, toks in enumerate(self.message_tokens):\n",
    "            filt = toks & valid\n",
    "            self.message_tokens[i] = filt\n",
    "            for t in filt:\n",
    "                self.token_index[t].add(i)\n",
    "\n",
    "        # prune edges to top_k\n",
    "        for u in list(self.graph.nodes()):\n",
    "            nbrs = [(v, self.graph[u][v]['weight']) for v in self.graph[u]]\n",
    "            nbrs.sort(key=lambda x: -x[1])\n",
    "            keep = {v for v, _ in nbrs[:self.top_k]}\n",
    "            for v in list(self.graph[u]):\n",
    "                if v not in keep:\n",
    "                    self.graph.remove_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32736c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('emails_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5641e7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7956</th>\n",
       "      <td>Re: RH 8 no DMA for DVD drive</td>\n",
       "      <td>Chris Kloiber &lt;ckloiber@ckloiber.com&gt;</td>\n",
       "      <td>rpm-zzzlist@freshrpms.net</td>\n",
       "      <td>Tue, 08 Oct 2002 23:23:19 -0400</td>\n",
       "      <td>On Tue, 2002-10-08 at 04:48, Panu Matilainen w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16752</th>\n",
       "      <td>Bosnia goes it alone for first full poll</td>\n",
       "      <td>guardian &lt;rssfeeds@example.com&gt;</td>\n",
       "      <td>yyyy@example.com</td>\n",
       "      <td>Sat, 05 Oct 2002 08:00:50 -0000</td>\n",
       "      <td>URL: http://www.newsisfree.com/click/-6,857278...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>try viagra for free</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>always wanted to try the drug the world has be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>[ILUG] Join the Web's Fastest Growing Singles ...</td>\n",
       "      <td>\"RankMyPix.com\" &lt;marjani@email2.qves.net&gt;</td>\n",
       "      <td>ilug@linux.ie</td>\n",
       "      <td>Fri, 23 Aug 2002 04:10:14 -0600</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>[Spambayes] Current histograms</td>\n",
       "      <td>tim.one@comcast.net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon, 09 Sep 2002 23:18:25 -0400</td>\n",
       "      <td>We've not only reduced the f-p and f-n rates i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 subject  \\\n",
       "7956                       Re: RH 8 no DMA for DVD drive   \n",
       "16752           Bosnia goes it alone for first full poll   \n",
       "12172                                try viagra for free   \n",
       "1681   [ILUG] Join the Web's Fastest Growing Singles ...   \n",
       "7489                      [Spambayes] Current histograms   \n",
       "\n",
       "                                            from                         to  \\\n",
       "7956       Chris Kloiber <ckloiber@ckloiber.com>  rpm-zzzlist@freshrpms.net   \n",
       "16752            guardian <rssfeeds@example.com>           yyyy@example.com   \n",
       "12172                                        NaN                        NaN   \n",
       "1681   \"RankMyPix.com\" <marjani@email2.qves.net>              ilug@linux.ie   \n",
       "7489                         tim.one@comcast.net                        NaN   \n",
       "\n",
       "                                  date  \\\n",
       "7956   Tue, 08 Oct 2002 23:23:19 -0400   \n",
       "16752  Sat, 05 Oct 2002 08:00:50 -0000   \n",
       "12172                              NaN   \n",
       "1681   Fri, 23 Aug 2002 04:10:14 -0600   \n",
       "7489   Mon, 09 Sep 2002 23:18:25 -0400   \n",
       "\n",
       "                                                    body  label  \n",
       "7956   On Tue, 2002-10-08 at 04:48, Panu Matilainen w...      0  \n",
       "16752  URL: http://www.newsisfree.com/click/-6,857278...      0  \n",
       "12172  always wanted to try the drug the world has be...      1  \n",
       "1681   1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1  \n",
       "7489   We've not only reduced the f-p and f-n rates i...      0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faed07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['body','label','subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e35ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d7a6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc=0.950, Prec=0.928, Rec=0.914, F1=0.921, Spec=0.967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,recall_score,precision_score\n",
    "\n",
    "graph_filter = GraphBasedSpamFilter(\n",
    "    similarity_threshold=0.2,\n",
    "    min_df=5,\n",
    "    max_df_ratio=0.7,\n",
    "    top_k=10\n",
    ")\n",
    "graph_filter.train_with_dataframe(train)\n",
    "\n",
    "# 4) Helper to extract metadata from a row\n",
    "def build_meta(row):\n",
    "    return {\n",
    "        \"from_domain\": row['from'].split('@')[-1] if pd.notnull(row['from']) and '@' in row['from'] else None,\n",
    "        \"to_count\": len(str(row['to']).split(',')) if pd.notnull(row['to']) else 0,\n",
    "        \"subject_length\": len(str(row['subject'] or \"\")),\n",
    "        \"has_attachment\": False,\n",
    "        \"hour\": pd.to_datetime(row['date'], errors='coerce').hour if pd.notnull(row['date']) else None,\n",
    "        \"weekday\": pd.to_datetime(row['date'], errors='coerce').weekday() if pd.notnull(row['date']) else None,\n",
    "        \"url_count\": len(re.findall(r'http[s]?://\\S+', str(row['body'] or \"\")))\n",
    "    }\n",
    "\n",
    "# 5) Predict on test set\n",
    "y_true = test['label'].tolist()\n",
    "y_pred = []\n",
    "for _, row in test.iterrows():\n",
    "    meta = build_meta(row)\n",
    "    y_pred.append(\n",
    "        graph_filter.predict_spam_subject_body(\n",
    "            row['subject'],\n",
    "            row['body'],\n",
    "            metadata=meta\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 6) Compute and print metrics\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "spec = tn / (tn + fp) if (tn + fp)>0 else 0\n",
    "\n",
    "print(f\"Acc={acc:.3f}, Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f}, Spec={spec:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
